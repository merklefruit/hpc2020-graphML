{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, plot_confusion_matrix, f1_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph import datasets\n",
    "from stellargraph.mapper import (\n",
    "    CorruptedGenerator,\n",
    "    FullBatchNodeGenerator,\n",
    "    GraphSAGENodeGenerator,\n",
    "    HinSAGENodeGenerator,\n",
    "    Node2VecNodeGenerator,\n",
    "    ClusterNodeGenerator,\n",
    ")\n",
    "from stellargraph.layer import GCN, DeepGraphInfomax, GraphSAGE, GAT, APPNP, HinSAGE, Dense\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import Model, optimizers, losses, metrics\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN, BorderlineSMOTE, KMeansSMOTE, SMOTENC, SVMSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import multiprocessing\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded. Loading it from file system\n",
      "LOADING DATA: 1.12 s\n",
      "PREPROCESSING: 1.11 s\n"
     ]
    }
   ],
   "source": [
    "v_data, e_data, v_sets, e_sets, core_targets, ext_targets, core_testing = utils.load_for_jupyter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account :\n",
      "-1.0    126863\n",
      " 0.0     13769\n",
      " 1.0      1244\n",
      "Name: testingFlag, dtype: int64\n",
      "Address :\n",
      "-1.0    28432\n",
      " 0.0     1568\n",
      "Name: testingFlag, dtype: int64\n",
      "Customer :\n",
      "-1.0    42127\n",
      " 0.0    13650\n",
      " 1.0      449\n",
      "Name: testingFlag, dtype: int64\n",
      "Derived Entity :\n",
      "-1.0    27286\n",
      " 0.0     3925\n",
      " 1.0       63\n",
      "Name: testingFlag, dtype: int64\n",
      "External Entity :\n",
      "-1.0    55207\n",
      " 0.0     4757\n",
      " 1.0       36\n",
      "Name: testingFlag, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sort based on testingFlag\n",
    "for i in v_sets:\n",
    "    v_sets[i] = v_sets[i].sort_values('testingFlag')\n",
    "    print(i,\":\")\n",
    "    print(v_sets[i].testingFlag.value_counts())\n",
    "    v_sets[i] = v_sets[i].drop('testingFlag', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in v_sets:\n",
    "    v_sets[i] = v_sets[i].drop('ExtendedCaseGraphID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample\n",
    "# do the embedding on only the nodes with testingFlag = 0 and the edges connected to those nodes\n",
    "\n",
    "sub_nodes = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'has account':                   source       target\n",
       "             edge_id                              \n",
       "             50000138943    100109621      1502000\n",
       "             50000138944    100109621      1502001\n",
       "             50000138945    100109621      1502002\n",
       "             500111591916   100109621   1502009415\n",
       "             500111630073   100109621  15020036336\n",
       "             ...                  ...          ...\n",
       "             500111612776  1001028763   1502008359\n",
       "             50000185720   1001028763  15020149211\n",
       "             50000185721   1001028763  15020151147\n",
       "             50000185722   1001028763  15020151148\n",
       "             50000185723   1001028763  15020151149\n",
       "             \n",
       "             [271876 rows x 2 columns],\n",
       "             'has address':                   source      target\n",
       "             edge_id                             \n",
       "             550111728302   100109622  2501120882\n",
       "             550111747300   100109623  2501127955\n",
       "             550111754789   100109624   250113982\n",
       "             550111743642   100109625  2501122656\n",
       "             550111740532   100109626   250119928\n",
       "             ...                  ...         ...\n",
       "             550111728619  1001028756  2501120934\n",
       "             550111740308  1001028757  2501119802\n",
       "             550111720500  1001028757  2501126785\n",
       "             550111729970  1001028760  2501120287\n",
       "             550111759169  1001028763  2501124955\n",
       "             \n",
       "             [40000 rows x 2 columns],\n",
       "             'is similar':                    source      target  weight\n",
       "             edge_id                                      \n",
       "             600111768422    100109621  3001181933       1\n",
       "             600111768033    100109631  3001184730       1\n",
       "             600111766073    100109641  3001157349       3\n",
       "             600111764661    100109650  3001163177       2\n",
       "             600111766520    100109651  3001139283       1\n",
       "             ...                   ...         ...     ...\n",
       "             650111831593  20030017545  3001182900       1\n",
       "             650111820399  20030017545  3001184369       2\n",
       "             650111814941  20030017545  3001147740       2\n",
       "             650111838272  20030017545  3001163228       2\n",
       "             650111856136  20030017545  3001149373       1\n",
       "             \n",
       "             [110000 rows x 3 columns],\n",
       "             'money transfer':                    source       target  weight\n",
       "             edge_id                                       \n",
       "             400111489790      1502000   2003003713      50\n",
       "             400111390345      1502000  20030012850       1\n",
       "             350111213741      1502001  15020066888       1\n",
       "             350111280215      1502001  15020063877      50\n",
       "             400111415408      1502001  20030019012       1\n",
       "             ...                   ...          ...     ...\n",
       "             450111534439  20030017541  15020038435       1\n",
       "             450111573483  20030017541  15020081346     500\n",
       "             450111546782  20030017543   1502001006      50\n",
       "             450111557835  20030017543  15020012252      50\n",
       "             450111496987  20030017545  15020088769       1\n",
       "             \n",
       "             [500000 rows x 3 columns]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "epochs = 15\n",
    "num_samples = [8, 4]\n",
    "dropout = 0.4\n",
    "hinsage_layer_sizes = [32, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = sg.StellarDiGraph(v_sets, e_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = HinSAGENodeGenerator(\n",
    "    G, \n",
    "    batch_size, \n",
    "    num_samples,\n",
    "    head_node_type=\"Account\"\n",
    ")\n",
    "\n",
    "def make_hinsage():\n",
    "    return HinSAGE(\n",
    "    layer_sizes=hinsage_layer_sizes,\n",
    "    activations=['relu', 'relu'],\n",
    "    generator=generator, \n",
    "    bias=True,\n",
    "    normalize=\"l2\",\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "hinsage = make_hinsage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_deep_graph_infomax(base_model, generator, epochs):\n",
    "    t0 = time.time()\n",
    "    corrupted_generator = CorruptedGenerator(generator)\n",
    "    gen = corrupted_generator.flow(G.nodes(node_type=\"Account\"))\n",
    "    infomax = DeepGraphInfomax(base_model, corrupted_generator)\n",
    "\n",
    "    x_in, x_out = infomax.in_out_tensors()\n",
    "\n",
    "    # Train DGI\n",
    "    model = Model(inputs=x_in, outputs=x_out)\n",
    "    model.compile(loss=tf.nn.sigmoid_cross_entropy_with_logits, optimizer=Adam(lr=1e-3))\n",
    "    es = EarlyStopping(monitor=\"loss\", min_delta=0, patience=15)\n",
    "    history = model.fit(gen, epochs=epochs, verbose=1, callbacks=[es])\n",
    "    sg.utils.plot_history(history)\n",
    "\n",
    "    x_emb_in, x_emb_out = base_model.in_out_tensors()\n",
    "    if generator.num_batch_dims() == 2:\n",
    "        x_emb_out = tf.squeeze(x_emb_out, axis=0)\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f'Time required: {t1-t0:.2f} s ({(t1-t0)/60:.1f} min)')\n",
    "    \n",
    "    return x_emb_in, x_emb_out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Deep Graph Infomax\n",
    "\n",
    "x_emb_in, x_emb_out, model = run_deep_graph_infomax(hinsage, generator, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
