{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, plot_confusion_matrix, f1_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph import datasets\n",
    "from stellargraph.mapper import (\n",
    "    CorruptedGenerator,\n",
    "    FullBatchNodeGenerator,\n",
    "    GraphSAGENodeGenerator,\n",
    "    HinSAGENodeGenerator,\n",
    "    Node2VecNodeGenerator,\n",
    "    ClusterNodeGenerator,\n",
    ")\n",
    "from stellargraph.layer import GCN, DeepGraphInfomax, GraphSAGE, GAT, APPNP, HinSAGE, Dense\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import Model, optimizers, losses, metrics\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN, BorderlineSMOTE, KMeansSMOTE, SMOTENC, SVMSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import multiprocessing\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded. Loading it from file system\n",
      "LOADING DATA: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "v_sets, e_sets, core_targets, ext_targets, core_testing = utils.load_for_jupyter_raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_sample = v_sets\n",
    "e_sample = e_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_sample.CoreCaseGraphID = v_sample.CoreCaseGraphID.fillna(0)\n",
    "v_sample.ExtendedCaseGraphID = v_sample.ExtendedCaseGraphID.fillna(0)\n",
    "v_sets = defaultdict()\n",
    "for v_type in list(pd.Categorical(v_sample.Label).categories):\n",
    "    v_sets[v_type] = v_sample[v_sample.Label == v_type]\n",
    "    v_sets[v_type] = v_sets[v_type].drop(['Label']+list(v_sets[v_type].columns[v_sets[v_type].isnull().all()]), axis=1)\n",
    "    v_sets[v_type].testingFlag = v_sets[v_type].testingFlag.fillna(-1)\n",
    "\n",
    "e_sets = defaultdict()\n",
    "for e_type in list(pd.Categorical(e_sample.Label).categories):\n",
    "    e_sets[e_type] = e_sample[e_sample.Label == e_type]\n",
    "    e_sets[e_type] = e_sets[e_type].drop(['Label']+list(e_sets[e_type].columns[e_sets[e_type].isnull().all()]), axis=1)\n",
    "    e_sets[e_type] = e_sets[e_type].rename(columns={'from_id':'source', 'to_id':'target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Revenue Size Flag</th>\n",
       "      <th>Account ID String</th>\n",
       "      <th>CoreCaseGraphID</th>\n",
       "      <th>ExtendedCaseGraphID</th>\n",
       "      <th>testingFlag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1502000</th>\n",
       "      <td>mid_high</td>\n",
       "      <td>RvIOFQqK0E</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502001</th>\n",
       "      <td>low</td>\n",
       "      <td>cSnM0hVDsm</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502002</th>\n",
       "      <td>low</td>\n",
       "      <td>WAQWpZi4AD</td>\n",
       "      <td>2492.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502003</th>\n",
       "      <td>mid_high</td>\n",
       "      <td>n5J9mBTeZc</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502004</th>\n",
       "      <td>low</td>\n",
       "      <td>qxlAEuUm7P</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020149208</th>\n",
       "      <td>low</td>\n",
       "      <td>upChD0y2n0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020149211</th>\n",
       "      <td>mid_high</td>\n",
       "      <td>73Tbarlxyj</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020151147</th>\n",
       "      <td>mid_low</td>\n",
       "      <td>L2aO11C2kD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020151148</th>\n",
       "      <td>mid_high</td>\n",
       "      <td>96mtdN1biG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020151149</th>\n",
       "      <td>mid_high</td>\n",
       "      <td>FNeTaqCQyo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141876 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Revenue Size Flag Account ID String  CoreCaseGraphID  \\\n",
       "node_id                                                            \n",
       "1502000              mid_high        RvIOFQqK0E              0.0   \n",
       "1502001                   low        cSnM0hVDsm              0.0   \n",
       "1502002                   low        WAQWpZi4AD           2492.0   \n",
       "1502003              mid_high        n5J9mBTeZc              0.0   \n",
       "1502004                   low        qxlAEuUm7P              0.0   \n",
       "...                       ...               ...              ...   \n",
       "15020149208               low        upChD0y2n0              0.0   \n",
       "15020149211          mid_high        73Tbarlxyj              0.0   \n",
       "15020151147           mid_low        L2aO11C2kD              0.0   \n",
       "15020151148          mid_high        96mtdN1biG              0.0   \n",
       "15020151149          mid_high        FNeTaqCQyo              0.0   \n",
       "\n",
       "             ExtendedCaseGraphID  testingFlag  \n",
       "node_id                                        \n",
       "1502000                      0.0         -1.0  \n",
       "1502001                      0.0         -1.0  \n",
       "1502002                      0.0          0.0  \n",
       "1502003                      0.0         -1.0  \n",
       "1502004                      0.0         -1.0  \n",
       "...                          ...          ...  \n",
       "15020149208                  0.0         -1.0  \n",
       "15020149211                  0.0         -1.0  \n",
       "15020151147                  0.0         -1.0  \n",
       "15020151148                  0.0         -1.0  \n",
       "15020151149                  0.0         -1.0  \n",
       "\n",
       "[141876 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_sets['Account']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#? 6: Additional Features\\n\\n# Adding 'Fraudolent' flag:\\nfor set in v_sets:\\n    v_sets[set]['Fraudolent'] = np.where(\\n    np.logical_or(v_sets[set]['CoreCaseGraphID'] != 0.0, v_sets[set]['ExtendedCaseGraphID'] != 0.0), '1', '0')\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finalize preprocessing\n",
    "\n",
    "#? 3: Logical conversion of categorical features\n",
    "\n",
    "#Revenue Size Flag: low, mid_low, medium, mid_high, high -> 1,2,3,4,5\n",
    "conversion = {'low':1, 'mid_low':2, 'medium':3, 'mid_high':4, 'high':5}\n",
    "for i in v_sets:\n",
    "    if 'Revenue Size Flag' in list(v_sets[i].columns):\n",
    "        v_sets[i]['Revenue Size Flag']=v_sets[i]['Revenue Size Flag'].map(conversion)\n",
    "\n",
    "#Income Size Flag: low, medium, high -> 1,2,3\n",
    "conversion = {'low':1, 'medium':2, 'high':3}\n",
    "for i in v_sets:\n",
    "    if 'Income Size Flag' in list(v_sets[i].columns):\n",
    "        v_sets[i]['Income Size Flag']=v_sets[i]['Income Size Flag'].map(conversion)\n",
    "\n",
    "#Similarity Strength: weak, medium, strong -> 1,2,3\n",
    "conversion = {'weak':1, 'medium':2, 'strong':3}\n",
    "for i in e_sets:\n",
    "    if 'Similarity Strength' in list(e_sets[i].columns):\n",
    "        e_sets[i]['Similarity Strength']= e_sets[i]['Similarity Strength'].map(conversion)\n",
    "        e_sets[i] = e_sets[i].rename(columns={'Similarity Strength':'weight'})\n",
    "\n",
    "#Amount Flag: small, medium, large -> 1,50,500 -> treated as weights\n",
    "conversion = {'small':1, 'medium':50, 'large':500}\n",
    "for i in e_sets:\n",
    "    if 'Amount Flag' in list(e_sets[i].columns):\n",
    "        e_sets[i]['Amount Flag']=e_sets[i]['Amount Flag'].map(conversion)\n",
    "        e_sets[i] = e_sets[i].rename(columns={'Amount Flag':'weight'})\n",
    "\n",
    "#? 4: One-hot encoding for categorical features\n",
    "\n",
    "# get_dummies for one-hot encoding\n",
    "for i in v_sets:\n",
    "    if 'Person or Organisation' in list(v_sets[i].columns):\n",
    "        v_sets[i] = pd.get_dummies(v_sets[i], columns=['Person or Organisation'])\n",
    "\n",
    "#? 5: String features\n",
    "\n",
    "# Attempt 1: remove them\n",
    "for i in v_sets:\n",
    "    if 'Account ID String' in list(v_sets[i].columns):\n",
    "        v_sets[i] = v_sets[i].drop('Account ID String', axis=1)\n",
    "    if 'Address' in list(v_sets[i].columns):\n",
    "        v_sets[i] = v_sets[i].drop('Address', axis=1)\n",
    "    if 'Name' in list(v_sets[i].columns):\n",
    "        v_sets[i] = v_sets[i].drop('Name', axis=1)\n",
    "\n",
    "'''\n",
    "#? 6: Additional Features\n",
    "\n",
    "# Adding 'Fraudolent' flag:\n",
    "for set in v_sets:\n",
    "    v_sets[set]['Fraudolent'] = np.where(\n",
    "    np.logical_or(v_sets[set]['CoreCaseGraphID'] != 0.0, v_sets[set]['ExtendedCaseGraphID'] != 0.0), '1', '0')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Revenue Size Flag</th>\n",
       "      <th>CoreCaseGraphID</th>\n",
       "      <th>ExtendedCaseGraphID</th>\n",
       "      <th>testingFlag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1502000</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502001</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502002</th>\n",
       "      <td>1</td>\n",
       "      <td>2492.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502003</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502004</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020149208</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020149211</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020151147</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020151148</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020151149</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141876 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Revenue Size Flag  CoreCaseGraphID  ExtendedCaseGraphID  \\\n",
       "node_id                                                                \n",
       "1502000                      4              0.0                  0.0   \n",
       "1502001                      1              0.0                  0.0   \n",
       "1502002                      1           2492.0                  0.0   \n",
       "1502003                      4              0.0                  0.0   \n",
       "1502004                      1              0.0                  0.0   \n",
       "...                        ...              ...                  ...   \n",
       "15020149208                  1              0.0                  0.0   \n",
       "15020149211                  4              0.0                  0.0   \n",
       "15020151147                  2              0.0                  0.0   \n",
       "15020151148                  4              0.0                  0.0   \n",
       "15020151149                  4              0.0                  0.0   \n",
       "\n",
       "             testingFlag  \n",
       "node_id                   \n",
       "1502000             -1.0  \n",
       "1502001             -1.0  \n",
       "1502002              0.0  \n",
       "1502003             -1.0  \n",
       "1502004             -1.0  \n",
       "...                  ...  \n",
       "15020149208         -1.0  \n",
       "15020149211         -1.0  \n",
       "15020151147         -1.0  \n",
       "15020151148         -1.0  \n",
       "15020151149         -1.0  \n",
       "\n",
       "[141876 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_sets['Account']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account :\n",
      "-1.0    126863\n",
      " 0.0     13769\n",
      " 1.0      1244\n",
      "Name: testingFlag, dtype: int64\n",
      "Address :\n",
      "-1.0    28432\n",
      " 0.0     1568\n",
      "Name: testingFlag, dtype: int64\n",
      "Customer :\n",
      "-1.0    42127\n",
      " 0.0    13650\n",
      " 1.0      449\n",
      "Name: testingFlag, dtype: int64\n",
      "Derived Entity :\n",
      "-1.0    27286\n",
      " 0.0     3925\n",
      " 1.0       63\n",
      "Name: testingFlag, dtype: int64\n",
      "External Entity :\n",
      "-1.0    55207\n",
      " 0.0     4757\n",
      " 1.0       36\n",
      "Name: testingFlag, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sort based on testingFlag\n",
    "for i in v_sets:\n",
    "    v_sets[i] = v_sets[i].sort_values('testingFlag')\n",
    "    print(i,\":\")\n",
    "    print(v_sets[i].testingFlag.value_counts())\n",
    "    v_sets[i] = v_sets[i].drop('testingFlag', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDEAS\n",
    "\n",
    "Order the dataset properly, remove the testingFlag, perform embeddings on all the nodes, take all the nodes with testingFlag = 0 and split in training and validation. Then test on nodes with testingFlag = 1.\n",
    "\n",
    "This can be done more simply by training the hinSAGE DeepGraphInfomax model on all the nodes, and performing in_out_tensors on the specific parts of dataset needed.\n",
    "\n",
    "1. Train HinSAGE with DGI on all nodes\n",
    "2. Embed nodes with testingFlag = 0\n",
    "3. Perform TSNE on them\n",
    "4. Split in training and validation (while keeping ordered)\n",
    "5. Train a classifier and evaluate performance\n",
    "6. Embed nodes with testingFlag = 1\n",
    "7. Perform TSNE on them and compare with training data TSNE\n",
    "8. Predict ext caseID on test dataset\n",
    "\n",
    "\n",
    "Actually, it's more useful to train and use the embedding only on the nodes with testingFlag set, because I need to find clusters of different cases, instead of clustering all the fraudolent nodes together...? To verify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, TODO: solve this: \n",
    "> NOTE: We have noticed little inconsistency between the testingFlag property and the info contained in the training and testing cases. To avoid any problem, we suggest to delete all the info included in the CoreCaseGraphID, ExtendedCaseGraphID, and testingFlag properties. Then you can import the correct information for the training cases (testingFlag == 0) from the training.core.vertices.csv and training.extended.vertices.csv files, and for the testing cases (testingFlag == 1) from the testing.core.vertices.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300\n",
    "epochs = 7\n",
    "num_samples = [8, 4]\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = sg.MultiDiGraph(v_sets, e_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ID, test_ID = model_selection.train_test_split(\n",
    "    ext_targets, train_size=0.7, test_size=0.3, #stratify=ext_targets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-09ac9e385918>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m generator = HinSAGENodeGenerator(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhead_node_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Account\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'G' is not defined"
     ]
    }
   ],
   "source": [
    "generator = HinSAGENodeGenerator(\n",
    "    G, \n",
    "    batch_size, \n",
    "    num_samples,\n",
    "    head_node_type=\"Account\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinsage_layer_sizes = [32, 32]\n",
    "assert len(hinsage_layer_sizes) == len(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinsage = HinSAGE(\n",
    "    layer_sizes=hinsage_layer_sizes,\n",
    "    activations=['relu', 'softmax'],\n",
    "    generator=generator, \n",
    "    bias=True,\n",
    "    normalize=\"l2\",\n",
    "    dropout=dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inp, x_out = hinsage.in_out_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_deep_graph_infomax(base_model, generator, epochs):\n",
    "    corrupted_generator = CorruptedGenerator(generator)\n",
    "    gen = corrupted_generator.flow(G.nodes(node_type=\"Account\"))\n",
    "    infomax = DeepGraphInfomax(base_model, corrupted_generator)\n",
    "\n",
    "    x_in, x_out = infomax.in_out_tensors()\n",
    "\n",
    "    # Train DGI\n",
    "    model = Model(inputs=x_in, outputs=x_out)\n",
    "    model.compile(loss=tf.nn.sigmoid_cross_entropy_with_logits, optimizer=Adam(lr=1e-3))\n",
    "    es = EarlyStopping(monitor=\"loss\", min_delta=0, patience=15)\n",
    "    history = model.fit(gen, epochs=epochs, verbose=1, callbacks=[es])\n",
    "    sg.utils.plot_history(history)\n",
    "\n",
    "    x_emb_in, x_emb_out = base_model.in_out_tensors()\n",
    "    if generator.num_batch_dims() == 2:\n",
    "        x_emb_out = tf.squeeze(x_emb_out, axis=0)\n",
    "\n",
    "    #do TSNE here\n",
    "    return x_emb_in, x_emb_out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run Deep Graph Infomax\n",
    "\n",
    "x_emb_in, x_emb_out, model = run_deep_graph_infomax(hinsage, generator, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only the accounts with testingFlag = 0 and embed them\n",
    "# aka remove the first 126863 and then take the first 13769\n",
    "# 15013\n",
    "\n",
    "v_sets['Account'][-15013:].CoreCaseGraphID.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_sets['Account'][-15013:].ExtendedCaseGraphID.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.7\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.scatter(\n",
    "    embeddings_2d[0],\n",
    "    embeddings_2d[1],\n",
    "    c=node_colours,\n",
    "    cmap=\"jet\",\n",
    "    alpha=alpha,\n",
    ")\n",
    "ax.set(aspect=\"equal\")\n",
    "plt.title(\"TSNE visualization of HinSAGE embeddings with Deep Graph Infomax\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloring based on ExtendedCaseGraphID\n",
    "node_ids = G.nodes(node_type=\"Account\").tolist()\n",
    "ext_targets = v_sample.loc[[int(node_id) for node_id in node_ids]].ExtendedCaseGraphID \n",
    "\n",
    "label_map = {l: i*10 for i, l in enumerate(np.unique(ext_targets), start=10) if pd.notna(l)}\n",
    "node_colours = [label_map[target] if pd.notna(target) else 0 for target in ext_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_2d = pd.DataFrame(TSNE(n_components=2).fit_transform(all_embeddings), index=G.nodes(node_type=\"Account\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = Model(inputs=x_emb_in, outputs=x_emb_out)\n",
    "all_embeddings = emb_model.predict(\n",
    "    #use a heterogeneous generator...\n",
    "    generator.flow(G.nodes(node_type=\"Account\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only nodes of type \"Account\" for prediction\n",
    "\n",
    "train_ID = train_ID[train_ID.index.isin(v_sets['Account'].index)]\n",
    "test_ID = test_ID[test_ID.index.isin(v_sets['Account'].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
