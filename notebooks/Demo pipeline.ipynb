{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import stellargraph as sg\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "from load_data import load_data\n",
    "v_data, e_data, core_target, ext_target, core_testing = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample dataset\n",
    "\n",
    "from tools import subsample\n",
    "v_sample, e_sample, core_sample, ext_sample = subsample(v_data, e_data, core_target, ext_target, 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Missing Core/Extended case ID\n",
    "# Attempt 1: set to 0 if NaN\n",
    "\n",
    "v_sample.CoreCaseGraphID = v_sample.CoreCaseGraphID.fillna(0)\n",
    "v_sample.ExtendedCaseGraphID = v_sample.ExtendedCaseGraphID.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many nodes I have\n",
    "\n",
    "print(f\"Nodes: {v_sample.count().values[0]} with {e_sample.count().values[0]} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes for each node type (Account, Customer, Derived entity, External entity, Address)\n",
    "# and each edge type (has account, has address, is similar, money transfer)\n",
    "\n",
    "v_sets = defaultdict()\n",
    "for v_type in list(pd.Categorical(v_sample.Label).categories):\n",
    "    v_sets[v_type] = v_sample[v_sample.Label == v_type]\n",
    "    v_sets[v_type] = v_sets[v_type].drop(['Label', 'testingFlag']+list(v_sets[v_type].columns[v_sets[v_type].isnull().all()]), axis=1)\n",
    "\n",
    "    e_sets = defaultdict()\n",
    "for e_type in list(pd.Categorical(e_sample.Label).categories):\n",
    "    e_sets[e_type] = e_sample[e_sample.Label == e_type]\n",
    "    e_sets[e_type] = e_sets[e_type].drop(['Label']+list(e_sets[e_type].columns[e_sets[e_type].isnull().all()]), axis=1)\n",
    "    e_sets[e_type] = e_sets[e_type].rename(columns={'from_id':'source', 'to_id':'target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Logical conversion from string to numeric (for categorical features)\n",
    "\n",
    "#Revenue Size Flag: low, mid_low, medium, mid_high, high -> 1,2,3,4,5\n",
    "conversion = {'low':1, 'mid_low':2, 'medium':3, 'mid_high':4, 'high':5}\n",
    "for i in v_sets:\n",
    "  if 'Revenue Size Flag' in list(v_sets[i].columns):\n",
    "    v_sets[i]['Revenue Size Flag']=v_sets[i]['Revenue Size Flag'].map(conversion)\n",
    "    \n",
    "#Income Size Flag: low, medium, high -> 1,2,3\n",
    "conversion = {'low':1, 'medium':2, 'high':3}\n",
    "for i in v_sets:\n",
    "  if 'Income Size Flag' in list(v_sets[i].columns):\n",
    "    v_sets[i]['Income Size Flag']=v_sets[i]['Income Size Flag'].map(conversion)\n",
    "\n",
    "#Similarity Strength: weak, medium, strong -> 1,2,3\n",
    "conversion = {'weak':1, 'medium':2, 'strong':3}\n",
    "for i in e_sets:\n",
    "  if 'Similarity Strength' in list(e_sets[i].columns):\n",
    "    e_sets[i]['Similarity Strength']= e_sets[i]['Similarity Strength'].map(conversion)\n",
    "    e_sets[i] = e_sets[i].rename(columns={'Similarity Strength':'weight'})\n",
    "\n",
    "#Amount Flag: small, medium, large -> 1,50,500 -> treated as weights\n",
    "conversion = {'small':1, 'medium':50, 'large':500}\n",
    "for i in e_sets:\n",
    "  if 'Amount Flag' in list(e_sets[i].columns):\n",
    "    e_sets[i]['Amount Flag']=e_sets[i]['Amount Flag'].map(conversion)\n",
    "    e_sets[i] = e_sets[i].rename(columns={'Amount Flag':'weight'})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: One-hot encoding for categorical features\n",
    "\n",
    "for i in v_sets:\n",
    "    if 'Person or Organisation' in list(v_sets[i].columns):\n",
    "        v_sets[i] = pd.get_dummies(v_sets[i], columns=['Person or Organisation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: String features\n",
    "# Attempt 1: remove them\n",
    "\n",
    "for i in v_sets:\n",
    "    if 'Account ID String' in list(v_sets[i].columns):\n",
    "      v_sets[i] = v_sets[i].drop('Account ID String', axis=1)\n",
    "    if 'Address' in list(v_sets[i].columns):\n",
    "      v_sets[i] = v_sets[i].drop('Address', axis=1)\n",
    "    if 'Name' in list(v_sets[i].columns):\n",
    "      v_sets[i] = v_sets[i].drop('Name', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StellarGraph setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = sg.StellarDiGraph(v_sets, e_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Graph embedding with NODE2VEC and WORD2VEC\n",
    "\n",
    "rw = sg.data.BiasedRandomWalk(G)\n",
    "t0 = time.time()\n",
    "walks = rw.run(\n",
    "    nodes=list(G.nodes()),  # root nodes\n",
    "    length=10,  # maximum length of a random walk\n",
    "    n=10,  # number of random walks per root node\n",
    "    p=0.6,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "    q=1.7,  # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    ")\n",
    "t1 = time.time()\n",
    "print(\"Number of random walks: {} in {:.2f} s\".format(len(walks), (t1-t0)))\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "str_walks = [[str(n) for n in walk] for walk in walks]\n",
    "model = Word2Vec(str_walks, size=128, window=5, min_count=0, sg=1, workers=8, iter=5)\n",
    "# size: length of embedding vector\n",
    "\n",
    "# The embedding vectors can be retrieved from model.wv using the node ID.\n",
    "# model.wv[\"19231\"].shape \n",
    "  \n",
    "# Retrieve node embeddings \n",
    "node_ids = model.wv.index2word  # list of node IDs\n",
    "node_embeddings = (model.wv.vectors)  # numpy.ndarray of size number of nodes times embeddings dimensionality\n",
    "\n",
    "# Retrieve corresponding targets\n",
    "\n",
    "# from training csv\n",
    "# core_targets = core_target_sample.loc[[int(node_id) for node_id in node_ids if int(node_id) in list(core_target_sample.index)]].CaseID\n",
    "# ext_targets = ext_target_sample.loc[[int(node_id) for node_id in node_ids if int(node_id) in list(ext_target_sample.index)]].CaseID\n",
    "\n",
    "# from vertices' data\n",
    "core_targets = v_sample.loc[[int(node_id) for node_id in node_ids]].CoreCaseGraphID\n",
    "ext_targets = v_sample.loc[[int(node_id) for node_id in node_ids]].ExtendedCaseGraphID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ext_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Take label of nodes by node_id\n",
    "\n",
    "tlab = time.time()\n",
    "labels = [None] * len(node_ids)\n",
    "for i in range(len(node_ids)):\n",
    "    labels[i] = v_sample.loc[[int(node_ids[i])]].Label.item()\n",
    "tlab2 = time.time()\n",
    "print(f\"Done {len(node_ids)} passes in {(tlab2-tlab):.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Transform the embeddings to 2d space for visualization\n",
    "transform = TSNE #PCA\n",
    "trans = transform(n_components=2)\n",
    "t2 = time.time()\n",
    "node_embeddings_2d_tsne = trans.fit_transform(node_embeddings)\n",
    "t3 = time.time()\n",
    "print(\"dimensionality reduction done in {:.2f} s\".format(t3-t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Draw the embedding points, coloring them by the target label (CaseID)\n",
    "alpha = 0.6\n",
    "label_map = {l: i for i, l in enumerate(np.unique(ext_targets), start=1000) if pd.notna(l)}\n",
    "label_map[0] = 1\n",
    "node_colours = [label_map[target] if pd.notna(target) else 0 for target in ext_targets]\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.axes().set(aspect=\"equal\")\n",
    "plt.scatter(\n",
    "    node_embeddings_2d_tsne[:, 0],\n",
    "    node_embeddings_2d_tsne[:, 1],\n",
    "    c=node_colours,\n",
    "    cmap=\"jet\",\n",
    "    alpha=alpha,\n",
    ")\n",
    "plt.title(\"{} visualization of node embeddings w.r.t. Extended Case ID\".format(transform.__name__))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Draw the embedding points, coloring them by node label\n",
    "alpha = 0.6\n",
    "label_map = {l: i*500 for i, l in enumerate(np.unique(labels), start=10) if pd.notna(l)}\n",
    "node_colours = [label_map[label] for label in labels]\n",
    "#node_colours = [label_map[target] if pd.notna(target) else 0 for target in ext_targets]\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.axes().set(aspect=\"equal\")\n",
    "plt.scatter(\n",
    "    node_embeddings_2d_tsne[:, 0],\n",
    "    node_embeddings_2d_tsne[:, 1],\n",
    "    c=node_colours,\n",
    "    cmap=\"BuGn\",\n",
    "    alpha=alpha,\n",
    ")\n",
    "plt.title(\"{} visualization of node embeddings w.r.t. Extended Case ID\".format(transform.__name__))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the embeddings to 2d space for visualization\n",
    "transform = PCA\n",
    "trans = transform(n_components=2)\n",
    "node_embeddings_2d_pca = trans.fit_transform(node_embeddings)\n",
    "\n",
    "# Draw the embedding points, coloring them by the target label (CaseID)\n",
    "alpha = 0.6\n",
    "label_map = {l: i for i, l in enumerate(np.unique(ext_targets), start=10) if pd.notna(l)}\n",
    "node_colours = [label_map[target] if pd.notna(target) else 0 for target in ext_targets]\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.axes().set(aspect=\"equal\")\n",
    "plt.scatter(\n",
    "    node_embeddings_2d_pca[:, 0],\n",
    "    node_embeddings_2d_pca[:, 1],\n",
    "    c=node_colours,\n",
    "    cmap=\"ocean\",\n",
    "    alpha=alpha,\n",
    ")\n",
    "plt.title(\"{} visualization of node embeddings w.r.t. Extended Case ID\".format(transform.__name__))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA here is useless\n",
    "# This approach produces very low quality embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
