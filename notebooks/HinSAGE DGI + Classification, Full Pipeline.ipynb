{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, plot_confusion_matrix, f1_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph import datasets\n",
    "from stellargraph.mapper import (\n",
    "    CorruptedGenerator,\n",
    "    FullBatchNodeGenerator,\n",
    "    GraphSAGENodeGenerator,\n",
    "    HinSAGENodeGenerator,\n",
    "    Node2VecNodeGenerator,\n",
    "    ClusterNodeGenerator,\n",
    ")\n",
    "from stellargraph.layer import GCN, DeepGraphInfomax, GraphSAGE, GAT, APPNP, HinSAGE, Dense\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import Model, optimizers, losses, metrics\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN, BorderlineSMOTE, KMeansSMOTE, SMOTENC, SVMSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import multiprocessing\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded. Loading it from file system\n",
      "LOADING DATA: 0.96 s\n"
     ]
    }
   ],
   "source": [
    "v_sets, e_sets, core_targets, ext_targets, core_testing = utils.load_for_jupyter_raw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Replace CoreCaseID and ExtCaseID with CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo\n",
    "v_sample = v_sets\n",
    "e_sample = e_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Add Extra Features: Node Degree (see Node Degree feature notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = e_sets.groupby('from_id').count().to_id\n",
    "source_data = pd.DataFrame(source_data)\n",
    "source_data = source_data.rename(columns={'to_id': 'source_degree'})\n",
    "source_data = source_data.rename_axis('node_id')\n",
    "\n",
    "target_data = e_sets.groupby('to_id').count().from_id\n",
    "target_data = pd.DataFrame(target_data)\n",
    "target_data = target_data.rename(columns={'from_id': 'target_degree'})\n",
    "target_data = target_data.rename_axis('node_id')\n",
    "\n",
    "v_sample = pd.merge(v_sample, source_data, left_index=True, right_index=True, how='left')\n",
    "v_sample = pd.merge(v_sample, target_data, left_index=True, right_index=True, how='left')\n",
    "\n",
    "v_sample['source_degree'] = v_sample['source_degree'].fillna(0)\n",
    "v_sample['target_degree'] = v_sample['target_degree'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing pipeline\n",
    "\n",
    "v_sample.CoreCaseGraphID = v_sample.CoreCaseGraphID.fillna(0)\n",
    "v_sample.ExtendedCaseGraphID = v_sample.ExtendedCaseGraphID.fillna(0)\n",
    "v_sets = defaultdict()\n",
    "for v_type in list(pd.Categorical(v_sample.Label).categories):\n",
    "    v_sets[v_type] = v_sample[v_sample.Label == v_type]\n",
    "    v_sets[v_type] = v_sets[v_type].drop(['Label']+list(v_sets[v_type].columns[v_sets[v_type].isnull().all()]), axis=1)\n",
    "    v_sets[v_type].testingFlag = v_sets[v_type].testingFlag.fillna(-1)\n",
    "\n",
    "e_sets = defaultdict()\n",
    "for e_type in list(pd.Categorical(e_sample.Label).categories):\n",
    "    e_sets[e_type] = e_sample[e_sample.Label == e_type]\n",
    "    e_sets[e_type] = e_sets[e_type].drop(['Label']+list(e_sets[e_type].columns[e_sets[e_type].isnull().all()]), axis=1)\n",
    "    e_sets[e_type] = e_sets[e_type].rename(columns={'from_id':'source', 'to_id':'target'})\n",
    "    \n",
    "#? 3: Logical conversion of categorical features\n",
    "\n",
    "#Revenue Size Flag: low, mid_low, medium, mid_high, high -> 1,2,3,4,5\n",
    "conversion = {'low':0.1, 'mid_low':0.3, 'medium':0.6, 'mid_high':0.8, 'high':1}\n",
    "for i in v_sets:\n",
    "    if 'Revenue Size Flag' in list(v_sets[i].columns):\n",
    "        v_sets[i]['Revenue Size Flag']=v_sets[i]['Revenue Size Flag'].map(conversion)\n",
    "\n",
    "#Income Size Flag: low, medium, high -> 1,2,3\n",
    "conversion = {'low':0.1, 'medium':0.5, 'high':1}\n",
    "for i in v_sets:\n",
    "    if 'Income Size Flag' in list(v_sets[i].columns):\n",
    "        v_sets[i]['Income Size Flag']=v_sets[i]['Income Size Flag'].map(conversion)\n",
    "\n",
    "#Similarity Strength: weak, medium, strong -> 1,2,3\n",
    "conversion = {'weak':0.1, 'medium':0.5, 'strong':1}\n",
    "for i in e_sets:\n",
    "    if 'Similarity Strength' in list(e_sets[i].columns):\n",
    "        e_sets[i]['Similarity Strength']= e_sets[i]['Similarity Strength'].map(conversion)\n",
    "        e_sets[i] = e_sets[i].rename(columns={'Similarity Strength':'weight'})\n",
    "\n",
    "#Amount Flag: small, medium, large -> 1,50,500 -> treated as weights\n",
    "conversion = {'small':0.1, 'medium':0.5, 'large':1}\n",
    "for i in e_sets:\n",
    "    if 'Amount Flag' in list(e_sets[i].columns):\n",
    "        e_sets[i]['Amount Flag']=e_sets[i]['Amount Flag'].map(conversion)\n",
    "        e_sets[i] = e_sets[i].rename(columns={'Amount Flag':'weight'})\n",
    "\n",
    "#? 4: One-hot encoding for categorical features\n",
    "\n",
    "# get_dummies for one-hot encoding\n",
    "for i in v_sets:\n",
    "    if 'Person or Organisation' in list(v_sets[i].columns):\n",
    "        v_sets[i] = pd.get_dummies(v_sets[i], columns=['Person or Organisation'])\n",
    "\n",
    "#? 5: String features\n",
    "\n",
    "# Attempt 1: remove them\n",
    "for i in v_sets:\n",
    "    if 'Account ID String' in list(v_sets[i].columns):\n",
    "        v_sets[i] = v_sets[i].drop('Account ID String', axis=1)\n",
    "    if 'Address' in list(v_sets[i].columns):\n",
    "        v_sets[i] = v_sets[i].drop('Address', axis=1)\n",
    "    if 'Name' in list(v_sets[i].columns):\n",
    "        v_sets[i] = v_sets[i].drop('Name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Revenue Size Flag</th>\n",
       "      <th>CoreCaseGraphID</th>\n",
       "      <th>ExtendedCaseGraphID</th>\n",
       "      <th>testingFlag</th>\n",
       "      <th>source_degree</th>\n",
       "      <th>target_degree</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1502000</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502001</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502002</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2492.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502003</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502004</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020149208</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020149211</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020151147</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020151148</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020151149</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141876 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Revenue Size Flag  CoreCaseGraphID  ExtendedCaseGraphID  \\\n",
       "node_id                                                                \n",
       "1502000                    0.8              0.0                  0.0   \n",
       "1502001                    0.1              0.0                  0.0   \n",
       "1502002                    0.1           2492.0                  0.0   \n",
       "1502003                    0.8              0.0                  0.0   \n",
       "1502004                    0.1              0.0                  0.0   \n",
       "...                        ...              ...                  ...   \n",
       "15020149208                0.1              0.0                  0.0   \n",
       "15020149211                0.8              0.0                  0.0   \n",
       "15020151147                0.3              0.0                  0.0   \n",
       "15020151148                0.8              0.0                  0.0   \n",
       "15020151149                0.8              0.0                  0.0   \n",
       "\n",
       "             testingFlag  source_degree  target_degree  \n",
       "node_id                                                 \n",
       "1502000             -1.0            2.0            7.0  \n",
       "1502001             -1.0            3.0            5.0  \n",
       "1502002              0.0            5.0            6.0  \n",
       "1502003             -1.0            5.0           11.0  \n",
       "1502004             -1.0            3.0            3.0  \n",
       "...                  ...            ...            ...  \n",
       "15020149208         -1.0            0.0            1.0  \n",
       "15020149211         -1.0            0.0            1.0  \n",
       "15020151147         -1.0            0.0            1.0  \n",
       "15020151148         -1.0            0.0            1.0  \n",
       "15020151149         -1.0            0.0            1.0  \n",
       "\n",
       "[141876 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_sets['Account']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account :\n",
      "-1.0    126863\n",
      " 0.0     13769\n",
      " 1.0      1244\n",
      "Name: testingFlag, dtype: int64\n",
      "Address :\n",
      "-1.0    28432\n",
      " 0.0     1568\n",
      "Name: testingFlag, dtype: int64\n",
      "Customer :\n",
      "-1.0    42127\n",
      " 0.0    13650\n",
      " 1.0      449\n",
      "Name: testingFlag, dtype: int64\n",
      "Derived Entity :\n",
      "-1.0    27286\n",
      " 0.0     3925\n",
      " 1.0       63\n",
      "Name: testingFlag, dtype: int64\n",
      "External Entity :\n",
      "-1.0    55207\n",
      " 0.0     4757\n",
      " 1.0       36\n",
      "Name: testingFlag, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sort based on testingFlag\n",
    "for i in v_sets:\n",
    "    v_sets[i] = v_sets[i].sort_values('testingFlag')\n",
    "    print(i,\":\")\n",
    "    print(v_sets[i].testingFlag.value_counts())\n",
    "    v_sets[i] = v_sets[i].drop('testingFlag', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Revenue Size Flag</th>\n",
       "      <th>CoreCaseGraphID</th>\n",
       "      <th>source_degree</th>\n",
       "      <th>target_degree</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1502000</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020057228</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020057227</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020057226</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020057225</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020070563</th>\n",
       "      <td>0.3</td>\n",
       "      <td>427.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502002233</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3549.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020052758</th>\n",
       "      <td>0.3</td>\n",
       "      <td>3573.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020135827</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3786.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020027847</th>\n",
       "      <td>0.6</td>\n",
       "      <td>3545.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141876 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Revenue Size Flag  CoreCaseGraphID  source_degree  target_degree\n",
       "node_id                                                                      \n",
       "1502000                    0.8              0.0            2.0            7.0\n",
       "15020057228                0.6              0.0            6.0            8.0\n",
       "15020057227                0.6              0.0            2.0            8.0\n",
       "15020057226                0.8              0.0            2.0            5.0\n",
       "15020057225                0.6              0.0            6.0            7.0\n",
       "...                        ...              ...            ...            ...\n",
       "15020070563                0.3            427.0            2.0            4.0\n",
       "1502002233                 0.1           3549.0            4.0            7.0\n",
       "15020052758                0.3           3573.0            4.0            2.0\n",
       "15020135827                0.1           3786.0            0.0            1.0\n",
       "15020027847                0.6           3545.0           10.0            9.0\n",
       "\n",
       "[141876 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing ExtendedCaseID:\n",
    "\n",
    "for i in v_sets:\n",
    "    v_sets[i] = v_sets[i].drop('ExtendedCaseGraphID', axis=1)\n",
    "\n",
    "v_sets['Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train HinSAGE on all the nodes\n",
    "\n",
    "Note: Embedding of the Accounts only for this stage. \n",
    "It'pretty easy to just repeat the process for other node categories and concatenate the results. For now I am trying with the Accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "epochs = 100\n",
    "num_samples = [8, 4]\n",
    "dropout = 0.7\n",
    "hinsage_layer_sizes = [32, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = sg.StellarDiGraph(v_sets, e_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = HinSAGENodeGenerator(\n",
    "    G, \n",
    "    batch_size, \n",
    "    num_samples,\n",
    "    head_node_type=\"Account\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinsage = HinSAGE(\n",
    "    layer_sizes=hinsage_layer_sizes,\n",
    "    activations=['relu', 'softmax'],\n",
    "    generator=generator, \n",
    "    bias=True,\n",
    "    normalize=\"l2\",\n",
    "    dropout=dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_deep_graph_infomax(base_model, generator, epochs):\n",
    "    t0 = time.time()\n",
    "    corrupted_generator = CorruptedGenerator(generator)\n",
    "    gen = corrupted_generator.flow(G.nodes(node_type=\"Account\"))\n",
    "    infomax = DeepGraphInfomax(base_model, corrupted_generator)\n",
    "\n",
    "    x_in, x_out = infomax.in_out_tensors()\n",
    "\n",
    "    # Train DGI\n",
    "    model = Model(inputs=x_in, outputs=x_out)\n",
    "    model.compile(loss=tf.nn.sigmoid_cross_entropy_with_logits, optimizer=Adam(lr=1e-3))\n",
    "    es = EarlyStopping(monitor=\"loss\", min_delta=0, patience=15)\n",
    "    history = model.fit(gen, epochs=epochs, verbose=1, callbacks=[es])\n",
    "    sg.utils.plot_history(history)\n",
    "\n",
    "    x_emb_in, x_emb_out = base_model.in_out_tensors()\n",
    "    if generator.num_batch_dims() == 2:\n",
    "        x_emb_out = tf.squeeze(x_emb_out, axis=0)\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f'Time required: {t1-t0:.2f} s ({(t1-t0)/60:.1f} min)')\n",
    "    \n",
    "    return x_emb_in, x_emb_out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "284/284 [==============================] - 124s 437ms/step - loss: 0.6509\n",
      "Epoch 2/100\n",
      "284/284 [==============================] - 119s 419ms/step - loss: 0.6087\n",
      "Epoch 3/100\n",
      "284/284 [==============================] - 119s 419ms/step - loss: 0.5792\n",
      "Epoch 4/100\n",
      "284/284 [==============================] - 118s 415ms/step - loss: 0.5569\n",
      "Epoch 5/100\n",
      "284/284 [==============================] - 118s 415ms/step - loss: 0.5397\n",
      "Epoch 6/100\n",
      "284/284 [==============================] - 118s 417ms/step - loss: 0.5168\n",
      "Epoch 7/100\n",
      "284/284 [==============================] - 118s 415ms/step - loss: 0.5020\n",
      "Epoch 8/100\n",
      "284/284 [==============================] - 121s 427ms/step - loss: 0.4922\n",
      "Epoch 9/100\n",
      "284/284 [==============================] - 138s 485ms/step - loss: 0.4841\n",
      "Epoch 10/100\n",
      "284/284 [==============================] - 127s 447ms/step - loss: 0.4773\n",
      "Epoch 11/100\n",
      "284/284 [==============================] - 130s 458ms/step - loss: 0.4684\n",
      "Epoch 12/100\n",
      "284/284 [==============================] - 126s 445ms/step - loss: 0.4484\n",
      "Epoch 13/100\n",
      "284/284 [==============================] - 131s 462ms/step - loss: 0.4228\n",
      "Epoch 14/100\n",
      "284/284 [==============================] - 127s 446ms/step - loss: 0.4095\n",
      "Epoch 15/100\n",
      "284/284 [==============================] - 117s 411ms/step - loss: 0.4018\n",
      "Epoch 16/100\n",
      "284/284 [==============================] - 115s 406ms/step - loss: 0.3964\n",
      "Epoch 17/100\n",
      "284/284 [==============================] - 115s 403ms/step - loss: 0.3907\n",
      "Epoch 18/100\n",
      "284/284 [==============================] - 122s 430ms/step - loss: 0.3868\n",
      "Epoch 19/100\n",
      "284/284 [==============================] - 120s 422ms/step - loss: 0.3810\n",
      "Epoch 20/100\n",
      "284/284 [==============================] - 116s 409ms/step - loss: 0.3760\n",
      "Epoch 21/100\n",
      "284/284 [==============================] - 117s 411ms/step - loss: 0.3711\n",
      "Epoch 22/100\n",
      "284/284 [==============================] - 116s 410ms/step - loss: 0.3695\n",
      "Epoch 23/100\n",
      "284/284 [==============================] - 116s 407ms/step - loss: 0.3654\n",
      "Epoch 24/100\n",
      "284/284 [==============================] - 116s 410ms/step - loss: 0.3649\n",
      "Epoch 25/100\n",
      "284/284 [==============================] - 119s 418ms/step - loss: 0.3607\n",
      "Epoch 26/100\n",
      "284/284 [==============================] - 120s 421ms/step - loss: 0.3584\n",
      "Epoch 27/100\n",
      "284/284 [==============================] - 119s 419ms/step - loss: 0.3558\n",
      "Epoch 28/100\n",
      "284/284 [==============================] - 119s 419ms/step - loss: 0.3490\n",
      "Epoch 29/100\n",
      "284/284 [==============================] - 119s 419ms/step - loss: 0.3423\n",
      "Epoch 30/100\n",
      "284/284 [==============================] - 118s 416ms/step - loss: 0.3334\n",
      "Epoch 31/100\n",
      "284/284 [==============================] - 118s 417ms/step - loss: 0.3309\n",
      "Epoch 32/100\n",
      "284/284 [==============================] - 119s 418ms/step - loss: 0.3324\n",
      "Epoch 33/100\n",
      "284/284 [==============================] - 118s 417ms/step - loss: 0.3306\n",
      "Epoch 34/100\n",
      "284/284 [==============================] - 122s 431ms/step - loss: 0.3281\n",
      "Epoch 35/100\n",
      "284/284 [==============================] - 123s 433ms/step - loss: 0.3267\n",
      "Epoch 36/100\n",
      "284/284 [==============================] - 122s 430ms/step - loss: 0.3248\n",
      "Epoch 37/100\n",
      "284/284 [==============================] - 123s 434ms/step - loss: 0.3248\n",
      "Epoch 38/100\n",
      "284/284 [==============================] - 123s 434ms/step - loss: 0.3240\n",
      "Epoch 39/100\n",
      "284/284 [==============================] - 122s 430ms/step - loss: 0.3207\n",
      "Epoch 40/100\n",
      "284/284 [==============================] - 120s 424ms/step - loss: 0.3203\n",
      "Epoch 41/100\n",
      "284/284 [==============================] - 121s 425ms/step - loss: 0.3194\n",
      "Epoch 42/100\n",
      "284/284 [==============================] - 120s 423ms/step - loss: 0.3146\n",
      "Epoch 43/100\n",
      "284/284 [==============================] - 121s 425ms/step - loss: 0.3130\n",
      "Epoch 44/100\n",
      "284/284 [==============================] - 120s 423ms/step - loss: 0.3119\n",
      "Epoch 45/100\n",
      "284/284 [==============================] - 120s 424ms/step - loss: 0.3102\n",
      "Epoch 46/100\n",
      "284/284 [==============================] - 121s 425ms/step - loss: 0.3097\n",
      "Epoch 47/100\n",
      "284/284 [==============================] - 131s 462ms/step - loss: 0.3099\n",
      "Epoch 48/100\n",
      "284/284 [==============================] - 118s 417ms/step - loss: 0.3062\n",
      "Epoch 49/100\n",
      "284/284 [==============================] - 118s 414ms/step - loss: 0.3041\n",
      "Epoch 50/100\n",
      "284/284 [==============================] - 117s 411ms/step - loss: 0.3025\n",
      "Epoch 51/100\n",
      "284/284 [==============================] - 118s 416ms/step - loss: 0.3013\n",
      "Epoch 52/100\n",
      "284/284 [==============================] - 2092s 7s/step - loss: 0.2999\n",
      "Epoch 53/100\n",
      "284/284 [==============================] - 203s 715ms/step - loss: 0.2988\n",
      "Epoch 54/100\n",
      "284/284 [==============================] - 195s 687ms/step - loss: 0.2973\n",
      "Epoch 55/100\n",
      "284/284 [==============================] - 224s 789ms/step - loss: 0.2938\n",
      "Epoch 56/100\n",
      "284/284 [==============================] - 221s 780ms/step - loss: 0.2936\n",
      "Epoch 57/100\n",
      "284/284 [==============================] - 227s 798ms/step - loss: 0.2920\n",
      "Epoch 58/100\n",
      "284/284 [==============================] - 215s 756ms/step - loss: 0.2912\n",
      "Epoch 59/100\n",
      "284/284 [==============================] - 23538s 83s/step - loss: 0.2915\n",
      "Epoch 60/100\n",
      "284/284 [==============================] - 6102s 21s/step - loss: 0.2900\n",
      "Epoch 61/100\n",
      "284/284 [==============================] - 120s 421ms/step - loss: 0.2884\n",
      "Epoch 62/100\n",
      "284/284 [==============================] - 118s 417ms/step - loss: 0.2885\n",
      "Epoch 63/100\n",
      "284/284 [==============================] - 130s 459ms/step - loss: 0.2872\n",
      "Epoch 64/100\n",
      "284/284 [==============================] - 133s 468ms/step - loss: 0.2857\n",
      "Epoch 65/100\n",
      "284/284 [==============================] - 132s 465ms/step - loss: 0.2829\n",
      "Epoch 66/100\n",
      "284/284 [==============================] - 129s 453ms/step - loss: 0.2845\n",
      "Epoch 67/100\n",
      "284/284 [==============================] - 126s 444ms/step - loss: 0.2826\n",
      "Epoch 68/100\n",
      "284/284 [==============================] - 134s 474ms/step - loss: 0.2848\n",
      "Epoch 69/100\n",
      "284/284 [==============================] - 133s 467ms/step - loss: 0.2819\n",
      "Epoch 70/100\n",
      "284/284 [==============================] - 130s 459ms/step - loss: 0.2824\n",
      "Epoch 71/100\n",
      "284/284 [==============================] - 127s 449ms/step - loss: 0.2828\n",
      "Epoch 72/100\n",
      "284/284 [==============================] - 124s 437ms/step - loss: 0.2815\n",
      "Epoch 73/100\n",
      " 40/284 [===>..........................] - ETA: 1:53 - loss: 0.2950"
     ]
    }
   ],
   "source": [
    "# Run Deep Graph Infomax\n",
    "\n",
    "x_emb_in, x_emb_out, model = run_deep_graph_infomax(hinsage, generator, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Use the model to predict the embedding of the training and cross validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# take the training + cv nodes from v_sets['Account']\n",
    "# aka the nodes with testingFlag = 0\n",
    "\n",
    "train_cv_set = v_sets['Account'][126863:126863+13769]\n",
    "train_cv_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform embeddings on them\n",
    "\n",
    "emb_model = Model(inputs=x_emb_in, outputs=x_emb_out)\n",
    "train_cv_embs = emb_model.predict(\n",
    "    generator.flow(train_cv_set.index.values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cv_embs[52]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TSNE on train + cv set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cv_embs_2d = pd.DataFrame(\n",
    "    TSNE(n_components=2).fit_transform(train_cv_embs), \n",
    "    index=train_cv_set.index.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloring based on ExtendedCaseGraphID\n",
    "\n",
    "# these are the training+cv indexes\n",
    "node_ids = train_cv_set.index.values.tolist()\n",
    "\n",
    "# these are the training+cv Extended case ID\n",
    "ext_targets_2 = v_sample.loc[[int(node_id) for node_id in node_ids]].ExtendedCaseGraphID \n",
    "\n",
    "label_map = {l: i*10 for i, l in enumerate(np.unique(ext_targets_2), start=10) if pd.notna(l)}\n",
    "node_colours = [label_map[target] if pd.notna(target) else 0 for target in ext_targets_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.7\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.scatter(\n",
    "    train_cv_embs_2d[0],\n",
    "    train_cv_embs_2d[1],\n",
    "    c=node_colours,\n",
    "    cmap=\"jet\",\n",
    "    alpha=alpha,\n",
    ")\n",
    "ax.set(aspect=\"equal\")\n",
    "plt.title(\"TSNE visualization of HinSAGE embeddings with Deep Graph Infomax - coloring on ExtendedCaseGraphID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node degree based coloring\n",
    "\n",
    "# these are the training+cv source degrees\n",
    "ext_targets_3 = v_sample.loc[[int(node_id) for node_id in node_ids]].source_degree\n",
    "\n",
    "label_map = {l: i*100 for i, l in enumerate(np.unique(ext_targets_3), start=10) if pd.notna(l)}\n",
    "node_colours = [label_map[target] if pd.notna(target) else 0 for target in ext_targets_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.7\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.scatter(\n",
    "    train_cv_embs_2d[0],\n",
    "    train_cv_embs_2d[1],\n",
    "    c=node_colours,\n",
    "    cmap=\"jet\",\n",
    "    alpha=alpha,\n",
    ")\n",
    "ax.set(aspect=\"equal\")\n",
    "plt.title(\"TSNE visualization of HinSAGE embeddings with Deep Graph Infomax - coloring based on node source degree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# account core case ID based coloring\n",
    "\n",
    "# these are the training+cv core case IDs\n",
    "ext_targets_5 = v_sample.loc[[int(node_id) for node_id in node_ids]]['CoreCaseGraphID']\n",
    "\n",
    "label_map = {l: i*100 for i, l in enumerate(np.unique(ext_targets_5), start=10) if pd.notna(l)}\n",
    "node_colours = [label_map[target] if pd.notna(target) else 0 for target in ext_targets_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.7\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.scatter(\n",
    "    train_cv_embs_2d[0],\n",
    "    train_cv_embs_2d[1],\n",
    "    c=node_colours,\n",
    "    cmap=\"jet\",\n",
    "    alpha=alpha,\n",
    ")\n",
    "ax.set(aspect=\"equal\")\n",
    "plt.title(\"TSNE visualization of HinSAGE embeddings with Deep Graph Infomax - coloring based on CoreCaseGraphID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create training and cross validation sets\n",
    "\n",
    "Note: I am not using fancy splitting methods since I want to keep track of the order of the nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very rudimentary and shitty splitting:\n",
    "\n",
    "n_embs = train_cv_embs.shape[0]\n",
    "\n",
    "train_set = train_cv_embs[:10000]\n",
    "train_labels = ext_targets_2.values[:10000]\n",
    "\n",
    "cv_set = train_cv_embs[-3769:]\n",
    "cv_labels = ext_targets_2.values[-3769:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_set is nothing more than the embedding of the account nodes of the training set.\n",
    "\n",
    "This means that I can get the ID of the first node just by incrementing the index of\n",
    "the train_set by 10000 and look at the train_cv_set dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be the first node in the train_set\n",
    "train_cv_set.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be its embeddings\n",
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same can be said about the CV set\n",
    "train_cv_set.iloc[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is correct, the extended ID of node 15020030225 must be 135.\n",
    "\n",
    "And the extended ID of node 15020041132 must be 3449."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_sample.loc[15020030225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v_sample.loc[15020041132]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirmed. The labels are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train a classifier to predict ExtendedGraphCaseID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'mnb': MultinomialNB(),\n",
    "    'gnb': GaussianNB(),\n",
    "    'svm1': SVC(kernel='linear'),\n",
    "    'svm2': SVC(kernel='rbf'),\n",
    "    'svm3': SVC(kernel='sigmoid'),\n",
    "    #'mlp1': MLPClassifier(),\n",
    "    #'mlp2': MLPClassifier(hidden_layer_sizes=[100, 100]),\n",
    "    'ada': AdaBoostClassifier(),\n",
    "    'dtc': DecisionTreeClassifier(),\n",
    "    'rfc': RandomForestClassifier(),\n",
    "    #'gbc': GradientBoostingClassifier(),\n",
    "    'lr': LogisticRegression(multi_class=\"auto\", solver=\"lbfgs\", max_iter=200)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = dict()\n",
    "accs = dict()\n",
    "\n",
    "for clf_name in classifiers:\n",
    "    t0 = time.time()\n",
    "    clf = classifiers[clf_name]\n",
    "    clf.fit(train_set, train_labels)\n",
    "    cv_pred = clf.predict(cv_set)\n",
    "    clf_f1 = f1_score(cv_labels, cv_pred, average='weighted')\n",
    "    clf_acc = (cv_pred == cv_labels).mean()\n",
    "    f1_scores[clf_name] = clf_f1\n",
    "    accs[clf_name] = clf_acc\n",
    "    t1 = time.time()\n",
    "    print(f\"Classifier: {clf_name} - f1: {clf_f1:.3f} - acc: {clf_acc:.3f} - time: {t1-t0:.0f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First optimizations\n",
    "\n",
    "The classes are extremely imbalanced. An easy technique could be to add class weight directly to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like a gradient boosting classifier works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "#cw = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "#sample_weights = compute_sample_weight('balanced', y=train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_weights = np.ones(10000)\n",
    "sample_weights[0] = 0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#classifier = LogisticRegression(\n",
    "#    verbose=0,\n",
    "#    solver='lbfgs',\n",
    "#    #max_iter=200,\n",
    "#)\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "classifier.fit(\n",
    "    X=train_set,\n",
    "    y=train_labels,\n",
    "    sample_weight=sample_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_pred = classifier.predict(cv_set)\n",
    "f1_avg = f1_score(cv_labels, cv_pred, average='micro')\n",
    "acc = (cv_pred == cv_labels).mean()\n",
    "\n",
    "print(f\"f1: {f1_avg:.3f} - acc: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = pd.crosstab(\n",
    "    cv_labels,\n",
    "    cv_pred,\n",
    "    rownames=['True'],\n",
    "    colnames=['Predicted'],\n",
    "    margins=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding why it sucks so bad\n",
    "\n",
    "Clearly, there are many problems.\n",
    "\n",
    "One of the main ones is the distributions of the labels. The data is severely imbalanced. It makes no sense to train a logistic classifier on data with a class that is 120x times the size of the second biggest class.\n",
    "\n",
    "Ways to overcome data imbalance for classification:\n",
    "\n",
    "- Super & Under sampling techniques, like SMOTE and ADASYN\n",
    "- XGBoost with tuned parameters for imbalanced classification (aka weight classes differently)\n",
    "- Ensembles: basically parallelizing classifiers that each can manage a small imbalance instead of having a strong imbalance on one classifier.\n",
    "\n",
    "Another problem is the quality of the embeddings: they are just bad. Some embedded vectors have really obvious forms like all 0's and only one dimension with non-zero value.\n",
    "\n",
    "And at last: Feature quality. I can't blame the embedding model for producing very bad embeddings when the graph it's using has basically 1 feature per node type. For instance, Account nodes only have the Revenue Size Flag and CoreCase ID. Not enough for producing good results. \n",
    "\n",
    "Let's take a look at the labels and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CV set labels (ext case)\n",
    "pd.DataFrame(cv_labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV set predictions\n",
    "pd.DataFrame(cv_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sparse confusion matrix visualization\n",
    "\n",
    "sns.set(rc={'figure.figsize':(6, 10)})\n",
    "plt.spy(confusion_matrix, precision = 0.1, markersize = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
