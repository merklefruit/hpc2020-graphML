{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, plot_confusion_matrix, f1_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph import datasets\n",
    "from stellargraph.mapper import (\n",
    "    CorruptedGenerator,\n",
    "    FullBatchNodeGenerator,\n",
    "    GraphSAGENodeGenerator,\n",
    "    HinSAGENodeGenerator,\n",
    "    Node2VecNodeGenerator,\n",
    "    ClusterNodeGenerator,\n",
    ")\n",
    "from stellargraph.layer import GCN, DeepGraphInfomax, GraphSAGE, GAT, APPNP, HinSAGE, Dense\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import Model, optimizers, losses, metrics\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN, BorderlineSMOTE, KMeansSMOTE, SMOTENC, SVMSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import multiprocessing\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded. Loading it from file system\n",
      "LOADING DATA: 1.04 s\n"
     ]
    }
   ],
   "source": [
    "v_sets, e_sets, core_targets, ext_targets, core_testing = utils.load_for_jupyter_raw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Replace CoreCaseID and ExtCaseID with CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo\n",
    "v_sample = v_sets\n",
    "e_sample = e_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Add Extra Features: Node Degree (see Node Degree feature notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = e_sets.groupby('from_id').count().to_id\n",
    "source_data = pd.DataFrame(source_data)\n",
    "source_data = source_data.rename(columns={'to_id': 'source_degree'})\n",
    "source_data = source_data.rename_axis('node_id')\n",
    "\n",
    "target_data = e_sets.groupby('to_id').count().from_id\n",
    "target_data = pd.DataFrame(target_data)\n",
    "target_data = target_data.rename(columns={'from_id': 'target_degree'})\n",
    "target_data = target_data.rename_axis('node_id')\n",
    "\n",
    "v_sample = pd.merge(v_sample, source_data, left_index=True, right_index=True, how='left')\n",
    "v_sample = pd.merge(v_sample, target_data, left_index=True, right_index=True, how='left')\n",
    "\n",
    "v_sample['source_degree'] = v_sample['source_degree'].fillna(0)\n",
    "v_sample['target_degree'] = v_sample['target_degree'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing pipeline\n",
    "\n",
    "v_sample.CoreCaseGraphID = v_sample.CoreCaseGraphID.fillna(0)\n",
    "v_sample.ExtendedCaseGraphID = v_sample.ExtendedCaseGraphID.fillna(0)\n",
    "v_sets = defaultdict()\n",
    "for v_type in list(pd.Categorical(v_sample.Label).categories):\n",
    "    v_sets[v_type] = v_sample[v_sample.Label == v_type]\n",
    "    v_sets[v_type] = v_sets[v_type].drop(['Label']+list(v_sets[v_type].columns[v_sets[v_type].isnull().all()]), axis=1)\n",
    "    v_sets[v_type].testingFlag = v_sets[v_type].testingFlag.fillna(-1)\n",
    "\n",
    "e_sets = defaultdict()\n",
    "for e_type in list(pd.Categorical(e_sample.Label).categories):\n",
    "    e_sets[e_type] = e_sample[e_sample.Label == e_type]\n",
    "    e_sets[e_type] = e_sets[e_type].drop(['Label']+list(e_sets[e_type].columns[e_sets[e_type].isnull().all()]), axis=1)\n",
    "    e_sets[e_type] = e_sets[e_type].rename(columns={'from_id':'source', 'to_id':'target'})\n",
    "    \n",
    "#? 3: Logical conversion of categorical features\n",
    "\n",
    "#Revenue Size Flag: low, mid_low, medium, mid_high, high -> 1,2,3,4,5\n",
    "conversion = {'low':0.1, 'mid_low':0.3, 'medium':0.6, 'mid_high':0.8, 'high':1}\n",
    "for i in v_sets:\n",
    "    if 'Revenue Size Flag' in list(v_sets[i].columns):\n",
    "        v_sets[i]['Revenue Size Flag']=v_sets[i]['Revenue Size Flag'].map(conversion)\n",
    "\n",
    "#Income Size Flag: low, medium, high -> 1,2,3\n",
    "conversion = {'low':0.1, 'medium':0.5, 'high':1}\n",
    "for i in v_sets:\n",
    "    if 'Income Size Flag' in list(v_sets[i].columns):\n",
    "        v_sets[i]['Income Size Flag']=v_sets[i]['Income Size Flag'].map(conversion)\n",
    "\n",
    "#Similarity Strength: weak, medium, strong -> 1,2,3\n",
    "conversion = {'weak':0.1, 'medium':0.5, 'strong':1}\n",
    "for i in e_sets:\n",
    "    if 'Similarity Strength' in list(e_sets[i].columns):\n",
    "        e_sets[i]['Similarity Strength']= e_sets[i]['Similarity Strength'].map(conversion)\n",
    "        e_sets[i] = e_sets[i].rename(columns={'Similarity Strength':'weight'})\n",
    "\n",
    "#Amount Flag: small, medium, large -> 1,50,500 -> treated as weights\n",
    "conversion = {'small':0.1, 'medium':0.5, 'large':1}\n",
    "for i in e_sets:\n",
    "    if 'Amount Flag' in list(e_sets[i].columns):\n",
    "        e_sets[i]['Amount Flag']=e_sets[i]['Amount Flag'].map(conversion)\n",
    "        e_sets[i] = e_sets[i].rename(columns={'Amount Flag':'weight'})\n",
    "\n",
    "#? 4: One-hot encoding for categorical features\n",
    "\n",
    "# get_dummies for one-hot encoding\n",
    "for i in v_sets:\n",
    "    if 'Person or Organisation' in list(v_sets[i].columns):\n",
    "        v_sets[i] = pd.get_dummies(v_sets[i], columns=['Person or Organisation'])\n",
    "\n",
    "#? 5: String features\n",
    "\n",
    "# Attempt 1: remove them\n",
    "for i in v_sets:\n",
    "    if 'Account ID String' in list(v_sets[i].columns):\n",
    "        v_sets[i] = v_sets[i].drop('Account ID String', axis=1)\n",
    "    if 'Address' in list(v_sets[i].columns):\n",
    "        v_sets[i] = v_sets[i].drop('Address', axis=1)\n",
    "    if 'Name' in list(v_sets[i].columns):\n",
    "        v_sets[i] = v_sets[i].drop('Name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Revenue Size Flag</th>\n",
       "      <th>CoreCaseGraphID</th>\n",
       "      <th>ExtendedCaseGraphID</th>\n",
       "      <th>testingFlag</th>\n",
       "      <th>source_degree</th>\n",
       "      <th>target_degree</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1502000</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502001</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502002</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2492.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502003</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502004</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020149208</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020149211</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020151147</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020151148</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020151149</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141876 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Revenue Size Flag  CoreCaseGraphID  ExtendedCaseGraphID  \\\n",
       "node_id                                                                \n",
       "1502000                    0.8              0.0                  0.0   \n",
       "1502001                    0.1              0.0                  0.0   \n",
       "1502002                    0.1           2492.0                  0.0   \n",
       "1502003                    0.8              0.0                  0.0   \n",
       "1502004                    0.1              0.0                  0.0   \n",
       "...                        ...              ...                  ...   \n",
       "15020149208                0.1              0.0                  0.0   \n",
       "15020149211                0.8              0.0                  0.0   \n",
       "15020151147                0.3              0.0                  0.0   \n",
       "15020151148                0.8              0.0                  0.0   \n",
       "15020151149                0.8              0.0                  0.0   \n",
       "\n",
       "             testingFlag  source_degree  target_degree  \n",
       "node_id                                                 \n",
       "1502000             -1.0            2.0            7.0  \n",
       "1502001             -1.0            3.0            5.0  \n",
       "1502002              0.0            5.0            6.0  \n",
       "1502003             -1.0            5.0           11.0  \n",
       "1502004             -1.0            3.0            3.0  \n",
       "...                  ...            ...            ...  \n",
       "15020149208         -1.0            0.0            1.0  \n",
       "15020149211         -1.0            0.0            1.0  \n",
       "15020151147         -1.0            0.0            1.0  \n",
       "15020151148         -1.0            0.0            1.0  \n",
       "15020151149         -1.0            0.0            1.0  \n",
       "\n",
       "[141876 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_sets['Account']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account :\n",
      "-1.0    126863\n",
      " 0.0     13769\n",
      " 1.0      1244\n",
      "Name: testingFlag, dtype: int64\n",
      "Address :\n",
      "-1.0    28432\n",
      " 0.0     1568\n",
      "Name: testingFlag, dtype: int64\n",
      "Customer :\n",
      "-1.0    42127\n",
      " 0.0    13650\n",
      " 1.0      449\n",
      "Name: testingFlag, dtype: int64\n",
      "Derived Entity :\n",
      "-1.0    27286\n",
      " 0.0     3925\n",
      " 1.0       63\n",
      "Name: testingFlag, dtype: int64\n",
      "External Entity :\n",
      "-1.0    55207\n",
      " 0.0     4757\n",
      " 1.0       36\n",
      "Name: testingFlag, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sort based on testingFlag\n",
    "for i in v_sets:\n",
    "    v_sets[i] = v_sets[i].sort_values('testingFlag')\n",
    "    print(i,\":\")\n",
    "    print(v_sets[i].testingFlag.value_counts())\n",
    "    v_sets[i] = v_sets[i].drop('testingFlag', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Revenue Size Flag</th>\n",
       "      <th>CoreCaseGraphID</th>\n",
       "      <th>source_degree</th>\n",
       "      <th>target_degree</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1502000</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020057228</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020057227</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020057226</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020057225</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020070563</th>\n",
       "      <td>0.3</td>\n",
       "      <td>427.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502002233</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3549.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020052758</th>\n",
       "      <td>0.3</td>\n",
       "      <td>3573.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020135827</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3786.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15020027847</th>\n",
       "      <td>0.6</td>\n",
       "      <td>3545.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141876 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Revenue Size Flag  CoreCaseGraphID  source_degree  target_degree\n",
       "node_id                                                                      \n",
       "1502000                    0.8              0.0            2.0            7.0\n",
       "15020057228                0.6              0.0            6.0            8.0\n",
       "15020057227                0.6              0.0            2.0            8.0\n",
       "15020057226                0.8              0.0            2.0            5.0\n",
       "15020057225                0.6              0.0            6.0            7.0\n",
       "...                        ...              ...            ...            ...\n",
       "15020070563                0.3            427.0            2.0            4.0\n",
       "1502002233                 0.1           3549.0            4.0            7.0\n",
       "15020052758                0.3           3573.0            4.0            2.0\n",
       "15020135827                0.1           3786.0            0.0            1.0\n",
       "15020027847                0.6           3545.0           10.0            9.0\n",
       "\n",
       "[141876 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing ExtendedCaseID:\n",
    "\n",
    "for i in v_sets:\n",
    "    v_sets[i] = v_sets[i].drop('ExtendedCaseGraphID', axis=1)\n",
    "\n",
    "v_sets['Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train HinSAGE on all the nodes\n",
    "\n",
    "Note: Embedding of the Accounts only for this stage. \n",
    "It'pretty easy to just repeat the process for other node categories and concatenate the results. For now I am trying with the Accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 400\n",
    "epochs = 100\n",
    "num_samples = [16, 8]\n",
    "dropout = 0.7\n",
    "hinsage_layer_sizes = [128, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = sg.StellarDiGraph(v_sets, e_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = HinSAGENodeGenerator(\n",
    "    G, \n",
    "    batch_size, \n",
    "    num_samples,\n",
    "    head_node_type=\"Account\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinsage = HinSAGE(\n",
    "    layer_sizes=hinsage_layer_sizes,\n",
    "    activations=['relu', 'softmax'],\n",
    "    generator=generator, \n",
    "    bias=True,\n",
    "    normalize=\"l2\",\n",
    "    dropout=dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_deep_graph_infomax(base_model, generator, epochs):\n",
    "    t0 = time.time()\n",
    "    corrupted_generator = CorruptedGenerator(generator)\n",
    "    gen = corrupted_generator.flow(G.nodes(node_type=\"Account\"))\n",
    "    infomax = DeepGraphInfomax(base_model, corrupted_generator)\n",
    "\n",
    "    x_in, x_out = infomax.in_out_tensors()\n",
    "\n",
    "    # Train DGI\n",
    "    model = Model(inputs=x_in, outputs=x_out)\n",
    "    model.compile(loss=tf.nn.sigmoid_cross_entropy_with_logits, optimizer=Adam(lr=1e-3))\n",
    "    es = EarlyStopping(monitor=\"loss\", min_delta=0, patience=15)\n",
    "    history = model.fit(gen, epochs=epochs, verbose=1, callbacks=[es])\n",
    "    sg.utils.plot_history(history)\n",
    "\n",
    "    x_emb_in, x_emb_out = base_model.in_out_tensors()\n",
    "    if generator.num_batch_dims() == 2:\n",
    "        x_emb_out = tf.squeeze(x_emb_out, axis=0)\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f'Time required: {t1-t0:.2f} s ({(t1-t0)/60:.1f} min)')\n",
    "    \n",
    "    return x_emb_in, x_emb_out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "355/355 [==============================] - 666s 2s/step - loss: 0.5201\n",
      "Epoch 2/100\n",
      " 49/355 [===>..........................] - ETA: 9:52 - loss: 0.4372"
     ]
    }
   ],
   "source": [
    "# Run Deep Graph Infomax\n",
    "\n",
    "x_emb_in, x_emb_out, model = run_deep_graph_infomax(hinsage, generator, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Use the model to predict the embedding of the training and cross validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# take the training + cv nodes from v_sets['Account']\n",
    "# aka the nodes with testingFlag = 0\n",
    "\n",
    "train_cv_set = v_sets['Account'][126863:126863+13769]\n",
    "train_cv_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform embeddings on them\n",
    "\n",
    "emb_model = Model(inputs=x_emb_in, outputs=x_emb_out)\n",
    "train_cv_embs = emb_model.predict(\n",
    "    generator.flow(train_cv_set.index.values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cv_embs[52]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TSNE on train + cv set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cv_embs_2d = pd.DataFrame(\n",
    "    TSNE(n_components=2).fit_transform(train_cv_embs), \n",
    "    index=train_cv_set.index.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloring based on ExtendedCaseGraphID\n",
    "\n",
    "# these are the training+cv indexes\n",
    "node_ids = train_cv_set.index.values.tolist()\n",
    "\n",
    "# these are the training+cv Extended case ID\n",
    "ext_targets_2 = v_sample.loc[[int(node_id) for node_id in node_ids]].ExtendedCaseGraphID \n",
    "\n",
    "label_map = {l: i*10 for i, l in enumerate(np.unique(ext_targets_2), start=10) if pd.notna(l)}\n",
    "node_colours = [label_map[target] if pd.notna(target) else 0 for target in ext_targets_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.7\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.scatter(\n",
    "    train_cv_embs_2d[0],\n",
    "    train_cv_embs_2d[1],\n",
    "    c=node_colours,\n",
    "    cmap=\"jet\",\n",
    "    alpha=alpha,\n",
    ")\n",
    "ax.set(aspect=\"equal\")\n",
    "plt.title(\"TSNE visualization of HinSAGE embeddings with Deep Graph Infomax - coloring on ExtendedCaseGraphID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node degree based coloring\n",
    "\n",
    "# these are the training+cv source degrees\n",
    "ext_targets_3 = v_sample.loc[[int(node_id) for node_id in node_ids]].source_degree\n",
    "\n",
    "label_map = {l: i*100 for i, l in enumerate(np.unique(ext_targets_3), start=10) if pd.notna(l)}\n",
    "node_colours = [label_map[target] if pd.notna(target) else 0 for target in ext_targets_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.7\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.scatter(\n",
    "    train_cv_embs_2d[0],\n",
    "    train_cv_embs_2d[1],\n",
    "    c=node_colours,\n",
    "    cmap=\"jet\",\n",
    "    alpha=alpha,\n",
    ")\n",
    "ax.set(aspect=\"equal\")\n",
    "plt.title(\"TSNE visualization of HinSAGE embeddings with Deep Graph Infomax - coloring based on node source degree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# account core case ID based coloring\n",
    "\n",
    "# these are the training+cv core case IDs\n",
    "ext_targets_5 = v_sample.loc[[int(node_id) for node_id in node_ids]]['CoreCaseGraphID']\n",
    "\n",
    "label_map = {l: i*100 for i, l in enumerate(np.unique(ext_targets_5), start=10) if pd.notna(l)}\n",
    "node_colours = [label_map[target] if pd.notna(target) else 0 for target in ext_targets_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.7\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.scatter(\n",
    "    train_cv_embs_2d[0],\n",
    "    train_cv_embs_2d[1],\n",
    "    c=node_colours,\n",
    "    cmap=\"jet\",\n",
    "    alpha=alpha,\n",
    ")\n",
    "ax.set(aspect=\"equal\")\n",
    "plt.title(\"TSNE visualization of HinSAGE embeddings with Deep Graph Infomax - coloring based on CoreCaseGraphID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create training and cross validation sets\n",
    "\n",
    "Note: I am not using fancy splitting methods since I want to keep track of the order of the nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very rudimentary and shitty splitting:\n",
    "\n",
    "n_embs = train_cv_embs.shape[0]\n",
    "\n",
    "train_set = train_cv_embs[:10000]\n",
    "train_labels = ext_targets_2.values[:10000]\n",
    "\n",
    "cv_set = train_cv_embs[-3769:]\n",
    "cv_labels = ext_targets_2.values[-3769:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_set is nothing more than the embedding of the account nodes of the training set.\n",
    "\n",
    "This means that I can get the ID of the first node just by incrementing the index of\n",
    "the train_set by 10000 and look at the train_cv_set dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be the first node in the train_set\n",
    "train_cv_set.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be its embeddings\n",
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same can be said about the CV set\n",
    "train_cv_set.iloc[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is correct, the extended ID of node 15020030225 must be 135.\n",
    "\n",
    "And the extended ID of node 15020041132 must be 3449."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_sample.loc[15020030225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v_sample.loc[15020041132]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirmed. The labels are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train a classifier to predict ExtendedGraphCaseID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'mnb': MultinomialNB(),\n",
    "    'gnb': GaussianNB(),\n",
    "    'svm1': SVC(kernel='linear'),\n",
    "    'svm2': SVC(kernel='rbf'),\n",
    "    'svm3': SVC(kernel='sigmoid'),\n",
    "    #'mlp1': MLPClassifier(),\n",
    "    #'mlp2': MLPClassifier(hidden_layer_sizes=[100, 100]),\n",
    "    'ada': AdaBoostClassifier(),\n",
    "    'dtc': DecisionTreeClassifier(),\n",
    "    'rfc': RandomForestClassifier(),\n",
    "    #'gbc': GradientBoostingClassifier(),\n",
    "    'lr': LogisticRegression(multi_class=\"auto\", solver=\"lbfgs\", max_iter=200)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = dict()\n",
    "accs = dict()\n",
    "def test_classifiers():\n",
    "    for clf_name in classifiers:\n",
    "        t0 = time.time()\n",
    "        clf = classifiers[clf_name]\n",
    "        clf.fit(train_set, train_labels)\n",
    "        cv_pred = clf.predict(cv_set)\n",
    "        clf_f1 = f1_score(cv_labels, cv_pred, average='weighted')\n",
    "        clf_acc = (cv_pred == cv_labels).mean()\n",
    "        f1_scores[clf_name] = clf_f1\n",
    "        accs[clf_name] = clf_acc\n",
    "        t1 = time.time()\n",
    "        print(f\"Classifier: {clf_name} - f1: {clf_f1:.3f} - acc: {clf_acc:.3f} - time: {t1-t0:.0f} s\")\n",
    "        \n",
    "# test_classifiers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First optimizations\n",
    "\n",
    "The classes are extremely imbalanced. An easy technique could be to add class weight directly to the model.\n",
    "\n",
    "Another option is to oversmaple with something like SMOTE or ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like a gradient boosting classifier works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight, compute_class_weight\n",
    "\n",
    "#cw = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "#sample_weights = compute_sample_weight('balanced', y=train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_weights = np.ones(10000)\n",
    "sample_weights[0] = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#classifier = LogisticRegression(\n",
    "#    verbose=0,\n",
    "#    solver='lbfgs',\n",
    "#    #max_iter=200,\n",
    "#)\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "classifier.fit(\n",
    "    X=train_set,\n",
    "    y=train_labels,\n",
    "    sample_weight=sample_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_pred = classifier.predict(cv_set)\n",
    "f1_avg = f1_score(cv_labels, cv_pred, average='micro')\n",
    "acc = (cv_pred == cv_labels).mean()\n",
    "\n",
    "print(f\"f1: {f1_avg:.3f} - acc: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = pd.crosstab(\n",
    "    cv_labels,\n",
    "    cv_pred,\n",
    "    rownames=['True'],\n",
    "    colnames=['Predicted'],\n",
    "    margins=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding why it sucks so bad\n",
    "\n",
    "Clearly, there are many problems.\n",
    "\n",
    "One of the main ones is the distributions of the labels. The data is severely imbalanced. It makes no sense to train a logistic classifier on data with a class that is 120x times the size of the second biggest class.\n",
    "\n",
    "Ways to overcome data imbalance for classification:\n",
    "\n",
    "- Super & Under sampling techniques, like SMOTE and ADASYN\n",
    "- XGBoost with tuned parameters for imbalanced classification (aka weight classes differently)\n",
    "- Ensembles: basically parallelizing classifiers that each can manage a small imbalance instead of having a strong imbalance on one classifier.\n",
    "\n",
    "Another problem is the quality of the embeddings: they are just bad. Some embedded vectors have really obvious forms like all 0's and only one dimension with non-zero value.\n",
    "\n",
    "And at last: Feature quality. I can't blame the embedding model for producing very bad embeddings when the graph it's using has basically 1 feature per node type. For instance, Account nodes only have the Revenue Size Flag and CoreCase ID. Not enough for producing good results. \n",
    "\n",
    "Let's take a look at the labels and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CV set labels (ext case)\n",
    "pd.DataFrame(cv_labels).value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV set predictions\n",
    "pd.DataFrame(cv_pred).value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sparse confusion matrix visualization\n",
    "\n",
    "sns.set(rc={'figure.figsize':(6, 10)})\n",
    "plt.spy(confusion_matrix, precision = 0.1, markersize = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Trying SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "\n",
    "classifier_2 = DecisionTreeClassifier()\n",
    "\n",
    "rs = RandomOverSampler()\n",
    "sm = SMOTE(random_state=42, sampling_strategy='minority') #ADASYN #SMOTE\n",
    "ts, tl = rs.fit_resample(train_set, train_labels)\n",
    "train_set_smoted, train_labels_smoted = sm.fit_resample(ts, tl)\n",
    "\n",
    "classifier_2.fit(\n",
    "    X=train_set_smoted,\n",
    "    y=train_labels_smoted,\n",
    "    #sample_weight=sample_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_pred_2 = classifier_2.predict(cv_set)\n",
    "f1_avg_2 = f1_score(cv_labels, cv_pred, average='micro')\n",
    "acc_2 = (cv_pred_2 == cv_labels).mean()\n",
    "\n",
    "print(f\"f1: {f1_avg_2:.3f} - acc: {acc_2:.3f}\")\n",
    "\n",
    "confusion_matrix_2 = pd.crosstab(\n",
    "    cv_labels,\n",
    "    cv_pred_2,\n",
    "    rownames=['True'],\n",
    "    colnames=['Predicted'],\n",
    "    margins=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(6, 10)})\n",
    "plt.spy(confusion_matrix_2, precision = 0.1, markersize = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knc = KNeighborsClassifier(algorithm='auto', n_jobs=-1)\n",
    "knc.fit(\n",
    "    train_set,\n",
    "    train_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_pred = knc.predict(cv_set)\n",
    "f1_avg_2 = f1_score(cv_labels, cv_pred, average='micro')\n",
    "acc_2 = (cv_pred == cv_labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_avg_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cv_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cv_pred).value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_2 = pd.crosstab(\n",
    "    cv_labels,\n",
    "    cv_pred,\n",
    "    rownames=['True'],\n",
    "    colnames=['Predicted'],\n",
    "    margins=True\n",
    ")\n",
    "sns.set(rc={'figure.figsize':(6, 10)})\n",
    "plt.spy(confusion_matrix_2, precision = 0.1, markersize = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
